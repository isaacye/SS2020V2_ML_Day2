{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SS20_lab2_LRm_skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWja+c++iBZzDyhzbQ3Tys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacye/SS2020V2_ML_Day2/blob/master/Session_2/SS20_lab2_LRm_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfdqdNA_bZo7",
        "colab_type": "text"
      },
      "source": [
        "# **Import Torch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVMg68nTbiXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #### Here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsCk9Jrh_QBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6q1QnMK_UCV",
        "colab_type": "text"
      },
      "source": [
        "##**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0aNL9wm_SS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_data = 2400\n",
        "x1 = np.random.rand(num_data) *10\n",
        "x2 = np.random.rand(num_data) *10\n",
        "e = np.random.normal(0, 0.5, num_data)\n",
        "X= np.array([x1,x2]).T  # T for transpose from (2, 2400) to (2400, 2)\n",
        "y=2*np.sin(x1) + np.log(0.5*x2**2)+e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iAcmku3_grY",
        "colab_type": "text"
      },
      "source": [
        "# **Data split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQArWOcd_bB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #### Here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xllKg8m2_thf",
        "colab_type": "text"
      },
      "source": [
        "##**Visualizating input data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvtdLcT9_jX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "ax1 = fig.add_subplot(1, 3, 1, projection='3d') # size 1 row, 3 col, location 1\n",
        "ax1.scatter(train_X[:, 0], train_X[:, 1], train_y, c=train_y, cmap='jet')\n",
        "\n",
        "ax1.set_xlabel('x1')\n",
        "ax1.set_ylabel('x2')\n",
        "ax1.set_zlabel('y')\n",
        "ax1.set_title('Train Set Distribution')\n",
        "ax1.set_zlim(-10, 6)  # z axis limit\n",
        "ax1.view_init(40, -60) #view angle\n",
        "ax1.invert_xaxis() #direction of number line\n",
        "\n",
        "ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
        "ax2.scatter(val_X[:, 0], val_X[:, 1], val_y, c=val_y, cmap='jet')\n",
        "\n",
        "ax2.set_xlabel('x1')\n",
        "ax2.set_ylabel('x2')\n",
        "ax2.set_zlabel('y')\n",
        "ax2.set_title('Validation Set Distribution')\n",
        "ax2.set_zlim(-10, 6)\n",
        "ax2.view_init(40, -60)\n",
        "ax2.invert_xaxis()\n",
        "\n",
        "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
        "ax3.scatter(test_X[:, 0], test_X[:, 1], test_y, c=test_y, cmap='jet')\n",
        "\n",
        "ax3.set_xlabel('x1')\n",
        "ax3.set_ylabel('x2')\n",
        "ax3.set_zlabel('y')\n",
        "ax3.set_title('Test Set Distribution')\n",
        "ax3.set_zlim(-10, 6)\n",
        "ax3.view_init(40, -60)\n",
        "ax3.invert_xaxis()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jTYutV4_5Zt",
        "colab_type": "text"
      },
      "source": [
        "##**Model(Hypothesis) Define**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVYLR6Li_yOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #### Here ###\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MswJsqaAByN",
        "colab_type": "text"
      },
      "source": [
        "##**Cost(Loss) Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kZrTxdf_9t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #### Here ###\n",
        "  reg_loss = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8P7kZ2AI8g",
        "colab_type": "text"
      },
      "source": [
        "##**Training & Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zEAqT6xAFe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #### Here ###\n",
        "\n",
        "model = LinearModel()\n",
        "\n",
        "lr = 0.005\n",
        "optimizer = optim.SGD(model.parameters(), lr =lr)  # model.parameters : W, b of linear model\n",
        "\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_mae = []\n",
        "list_mae_epoch = []\n",
        "\n",
        "epoch = 4000\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "  # ===== Training ===== #\n",
        " #### Here ###\n",
        "# Setting mode for model train\n",
        "# initialize gradient\n",
        "\n",
        "  input_x =\n",
        "  true_y =\n",
        "  pred_y = model(input_x)\n",
        "\n",
        "  loss = reg_loss()\n",
        "   # backward() calculate gradients\n",
        "   # update gradients using step()\n",
        "  list_epoch.append(i)\n",
        "  list_train_loss.append(loss.detach().numpy()) #taking only loss value using detach\n",
        "\n",
        "  # ===== Validation =====#\n",
        "    #### Here ####\n",
        "# Setting mode for model train\n",
        "# initialize gradient\n",
        "  input_x =\n",
        "  true_y =\n",
        "  pred_y = model(input_x)\n",
        "\n",
        "  loss = reg_loss()\n",
        "  list_val_loss.append(loss.detach().numpy()) #taking only loss value using detach\n",
        "\n",
        "\n",
        "  # ====== Evaluation ======= #\n",
        "  \n",
        "  if i % 200 == 0: \n",
        "        \n",
        "        # ====== Calculate Error ====== #\n",
        "        #### Here ####\n",
        "        # Setting mode for model train\n",
        "        # initialize gradient\n",
        "\n",
        "        input_x =\n",
        "        true_y =\n",
        "        pred_y = model(input_x).detach().numpy() #to use MAE\n",
        "        mae = mean_absolute_error(true_y, pred_y)\n",
        "        list_mae.append(mae)\n",
        "        list_mae_epoch.append(i)\n",
        "\n",
        "        loss = reg_loss()\n",
        "        # backward() calculate gradients\n",
        "        # update gradients using step()\n",
        "        list_epoch.append(i)\n",
        "        list_train_loss.append(loss.detach().numpy()) #taking only loss value using detach\n",
        "\n",
        "        #### Here ####\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        fig = plt.figure(figsize=(15,5))\n",
        "        \n",
        "        # ====== True Y Scattering ====== #\n",
        "        ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
        "        ax1.scatter(test_X[:, 0], test_X[:, 1], test_y, c=test_y, cmap='jet')\n",
        "        \n",
        "        ax1.set_xlabel('x1')\n",
        "        ax1.set_ylabel('x2')\n",
        "        ax1.set_zlabel('y')\n",
        "        ax1.set_zlim(-10, 6)\n",
        "        ax1.view_init(40, -40)\n",
        "        ax1.set_title('True test y')\n",
        "        ax1.invert_xaxis()\n",
        "\n",
        "        # ====== Predicted Y Scattering ====== #\n",
        "        ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
        "        ax2.scatter(test_X[:, 0], test_X[:, 1], pred_y, c=pred_y[:,0], cmap='jet')\n",
        "\n",
        "        ax2.set_xlabel('x1')\n",
        "        ax2.set_ylabel('x2')\n",
        "        ax2.set_zlabel('y')\n",
        "        ax2.set_zlim(-10, 6)\n",
        "        ax2.view_init(40, -40)\n",
        "        ax2.set_title('Predicted test y')\n",
        "        ax2.invert_xaxis()\n",
        "\n",
        "        # ====== Just for Visualizaing with High Resolution ====== #\n",
        "        input_x = torch.Tensor(train_X)\n",
        "        pred_y = model(input_x).detach().numpy() \n",
        "        \n",
        "        ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
        "        ax3.scatter(train_X[:, 0], train_X[:, 1], pred_y, c=pred_y[:,0], cmap='jet')\n",
        "\n",
        "        ax3.set_xlabel('x1')\n",
        "        ax3.set_ylabel('x2')\n",
        "        ax3.set_zlabel('y')\n",
        "        ax3.set_zlim(-10, 6)\n",
        "        ax3.view_init(40, -40)\n",
        "        ax3.set_title('Predicted train y')\n",
        "        ax3.invert_xaxis()\n",
        "        \n",
        "        plt.show()\n",
        "        print(i, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZAsR0EJAXKS",
        "colab_type": "text"
      },
      "source": [
        "##**Presenting loss and error**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-u1yTm-ATqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "# ====== Loss Fluctuation ====== #\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
        "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.set_ylim(0, 5)\n",
        "ax1.grid()\n",
        "ax1.legend()\n",
        "ax1.set_title('epoch vs loss')\n",
        "\n",
        "# ====== Metric Fluctuation ====== #\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.plot(list_mae_epoch, list_mae, marker='x', label='mae metric')\n",
        "\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('mae')\n",
        "ax2.grid()\n",
        "ax2.legend()\n",
        "ax2.set_title('epoch vs mae')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}